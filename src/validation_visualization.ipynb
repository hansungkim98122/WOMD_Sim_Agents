{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4pgCuuygWGH"
   },
   "source": [
    "# Waymo Open Sim Agents Challenge Tutorial ðŸš—\n",
    "\n",
    "Follow along the\n",
    "[Sim Agents Challenge web page](https://waymo.com/open/challenges/2025/sim-agents)\n",
    "for more details.\n",
    "\n",
    "This tutorial demonstrates:\n",
    "\n",
    "- How to load the motion dataset.\n",
    "- How to simulate a rollout (as specified by the challenge) from a single scenario and a simple policy.\n",
    "- How to visualize the results.\n",
    "- How to evaluate the simulation locally.\n",
    "- How to package the simulations into the protobuf used for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y13_DSHa3bFN"
   },
   "source": [
    "## Package installation ðŸ› ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3qW_kdSgTM5"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "# Set matplotlib to jshtml so animations work with colab.\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from utils.geometry import wrap_angle\n",
    "\n",
    "from waymo_open_dataset.protos import scenario_pb2\n",
    "from waymo_open_dataset.protos import sim_agents_submission_pb2\n",
    "from waymo_open_dataset.utils import trajectory_utils\n",
    "from waymo_open_dataset.utils.sim_agents import submission_specs\n",
    "from waymo_open_dataset.utils.sim_agents import visualizations\n",
    "from waymo_open_dataset.wdl_limited.sim_agents_metrics import metric_features\n",
    "from waymo_open_dataset.wdl_limited.sim_agents_metrics import metrics\n",
    "\n",
    "rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f23GbrgV3naf"
   },
   "source": [
    "# Loading the data\n",
    "\n",
    "Visit the [Waymo Open Dataset Website](https://waymo.com/open/) to download the\n",
    "full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sk0njhKl3yWX"
   },
   "outputs": [],
   "source": [
    "# Please edit.\n",
    "\n",
    "# Replace this path with your own tfrecords./\n",
    "# This tutorial is based on using data in the Scenario proto format directly,\n",
    "# so choose the correct dataset version.\n",
    "VAL_DATASET_FOLDER = '/home/hansung/end2end_ad/datasets/waymo_open_dataset_motion_v_1_3_0/uncompressed/scenario/validation/'\n",
    "TEST_DATASET_FOLDER = '/home/hansung/end2end_ad/datasets/waymo_open_dataset_motion_v_1_3_0/uncompressed/scenario/testing/'\n",
    "VALIDATION_FILES = os.path.join(VAL_DATASET_FOLDER, 'validation.tfrecord*')\n",
    "TEST_FILES = os.path.join(TEST_DATASET_FOLDER, 'testing.tfrecord*')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDdThI324gwG"
   },
   "source": [
    "We create a dataset starting from the validation set, which is smaller than the\n",
    "training set but contains future states (which the test set does not). We need\n",
    "future information to demonstrate how to evaluate your submission locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbXocuUZ4qgH"
   },
   "outputs": [],
   "source": [
    "# Define the dataset from the TFRecords.\n",
    "dataset_mode = 'val' #[val,test]\n",
    "if dataset_mode == 'val':\n",
    "    filenames = tf.io.matching_files(VALIDATION_FILES)\n",
    "else: \n",
    "    filenames = tf.io.matching_files(TEST_FILES)\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "# Since these are raw Scenario protos, we need to parse them in eager mode.\n",
    "dataset_iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_example = next(dataset_iterator)\n",
    "scenario = scenario_pb2.Scenario.FromString(bytes_example)\n",
    "id = scenario.scenario_id\n",
    "\n",
    "print(f'Checking type: {type(scenario)}')\n",
    "print(f'Loaded scenario with ID: {scenario.scenario_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eybmOjww52Me"
   },
   "source": [
    "# Simulation stage ðŸ¤–\n",
    "\n",
    "Please read the\n",
    "[challenge web page](https://waymo.com/open/challenges/2025/sim-agents) first,\n",
    "where we explain simulation requirements and settings.\n",
    "\n",
    "Many of the requirements specified on the challenge website are encoded into\n",
    "`waymo_open_dataset/utils/sim_agents/submission_specs.py`. For example, we have\n",
    "specifications of: - Simulation length and frequency. - Number of parallel\n",
    "simulations required. - Agents to simulate and agents to evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObeYHuJbBJuO"
   },
   "source": [
    "### What to simulate\n",
    "\n",
    "There are multiple levels of fidelity we could ask to perform these simulations,\n",
    "but we are going to follow a very similar abstraction used for the Waymo Open\n",
    "Motion Dataset: we represent objects as boxes, and we are interested just in how\n",
    "they *move* around the world.\n",
    "\n",
    "Specifically, contestants will need to simulate the fields specified in the\n",
    "`sim_agents_submission_pb2.SimulatedTrajectory` proto, namely:\n",
    "- 3D coordinates of the boxes centers (x/y/z, same reference frame as the original Scenario).\n",
    "- Heading of those objects (again, same definition as the original Scenario proto).\n",
    "\n",
    "All the objects that are valid at the last step of the initial state (i.e. the\n",
    "11th when 1-indexed, the last observable one in the test set data) needs to be\n",
    "resimulated. We provide a simple util function (shown below) to identify who\n",
    "needs to be resimulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4UE3NY9L7AO"
   },
   "source": [
    "To demonstrate how a simulation needs to be carried out, we implement a very\n",
    "trivial policy for our sim agents, i.e. a linear extrapolation of their past\n",
    "trajectory, at constant speed. Since these agents will not be reactive, this\n",
    "will result in a bad score in the final evaluation (more details below).\n",
    "\n",
    "We want to highlight 2 main properties of the simulation: - The simulation is\n",
    "carried on in a **closed-loop** fashion, iterating over the steps and having the\n",
    "policies \"observe\" up to that step the action of the others. - These policies\n",
    "are not sharing any information about the future intentions of each other. This\n",
    "is not strictly required for every agent except the AV, but it is required\n",
    "between the AV and everyone else (because it will be swapped with a\n",
    "non-controlled agent when testing in an actual AV simulator).\n",
    "\n",
    "For more details refer to the\n",
    "[challenge's web page](https://waymo.com/open/challenges/2025/sim-agents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_type = submission_specs.ChallengeType.SIM_AGENTS\n",
    "submission_config = submission_specs.get_submission_config(challenge_type)\n",
    "\n",
    "print(f'Simulation length, in steps: {submission_config.n_simulation_steps}')\n",
    "print(\n",
    "    'Duration of a step, in seconds:'\n",
    "    f' {submission_config.step_duration_seconds}s (frequency:'\n",
    "    f' {1/submission_config.step_duration_seconds}Hz)'\n",
    ")\n",
    "print(\n",
    "    'Number of parallel simulations per Scenario:'\n",
    "    f' {submission_config.n_rollouts}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ```rl_finetune.py```, you can correct the checkpoint keys using the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "in_ckpt  = \"checkpoints/finetuning/last.ckpt\"\n",
    "out_ckpt = \"checkpoints/finetuning/last_mapkeys_fixed.ckpt\"\n",
    "\n",
    "ckpt = torch.load(in_ckpt,map_location='cpu',weights_only=False)\n",
    "sd = ckpt[\"state_dict\"]\n",
    "\n",
    "new_sd = {}\n",
    "for k, v in sd.items():\n",
    "    # Only rewrite map_encoder keys\n",
    "    if k.startswith(\"model.\"):\n",
    "        k2 = k.replace(\"model.\", \"\", 1)  # remove ONLY the first \"model.\"\n",
    "        new_sd[k2] = v\n",
    "    else:\n",
    "        new_sd[k] = v\n",
    "\n",
    "ckpt[\"state_dict\"] = new_sd\n",
    "torch.save(ckpt, out_ckpt)\n",
    "\n",
    "print(\"saved:\", out_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scenario.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "visualizations.add_map(ax, scenario)\n",
    "\n",
    "\n",
    "def plot_track_trajectory(track: scenario_pb2.Track) -> None:\n",
    "  valids = np.array([state.valid for state in track.states])\n",
    "  if np.any(valids):\n",
    "    x = np.array([state.center_x for state in track.states])\n",
    "    y = np.array([state.center_y for state in track.states])\n",
    "    ax.plot(x[valids], y[valids], linewidth=5)\n",
    "\n",
    "\n",
    "for track in scenario.tracks:\n",
    "  plot_track_trajectory(track)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout (Simulation using our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "from dataset.dataset import MultiDataset\n",
    "from model.smart import SMART\n",
    "from utils.config import load_config_act\n",
    "from transforms.target_builder import WaymoTargetBuilder\n",
    "import torch\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "def smooth_upsample_knots_to_0p1(traj, shift=5, dt=0.1):\n",
    "    \"\"\"\n",
    "    traj: (num_agents, T+1, 4) with knot values filled at indices 0, shift, 2*shift, ...\n",
    "          columns: [x, y, z, yaw]\n",
    "    Returns: same shape (num_agents, T+1, 4) fully filled with smooth 0.1s samples.\n",
    "    \"\"\"\n",
    "    num_agents, Tp1, D = traj.shape\n",
    "    assert D == 4\n",
    "    T = Tp1 - 1\n",
    "\n",
    "    knot_idx = np.arange(0, Tp1, shift)          # e.g. [0,5,10,...,80]\n",
    "    t_k = knot_idx * dt\n",
    "    t = np.arange(0, Tp1) * dt\n",
    "\n",
    "    out = traj.copy()\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        # --- x,y,z: C2-smooth spline (smooth velocity + acceleration) ---\n",
    "        for dim in (0, 1, 2):\n",
    "            yk = traj[i, knot_idx, dim]\n",
    "            cs = CubicSpline(t_k, yk, bc_type=\"natural\")\n",
    "            out[i, :, dim] = cs(t)\n",
    "\n",
    "        # --- yaw: spline sin/cos to avoid wrap discontinuity ---\n",
    "        yaw_k = traj[i, knot_idx, 3]\n",
    "        s_k = np.sin(yaw_k)\n",
    "        c_k = np.cos(yaw_k)\n",
    "\n",
    "        cs_s = CubicSpline(t_k, s_k, bc_type=\"natural\")\n",
    "        cs_c = CubicSpline(t_k, c_k, bc_type=\"natural\")\n",
    "\n",
    "        s = cs_s(t)\n",
    "        c = cs_c(t)\n",
    "\n",
    "        # normalize (keeps it on unit circle; helps avoid drift)\n",
    "        norm = np.maximum(np.sqrt(s*s + c*c), 1e-8)\n",
    "        s /= norm\n",
    "        c /= norm\n",
    "\n",
    "        out[i, :, 3] = np.arctan2(s, c)\n",
    "\n",
    "    return out\n",
    "\n",
    "def interp_angle_shortest(a0, a1, alpha):\n",
    "        # a0,a1: (...,) in radians, alpha scalar in [0,1]\n",
    "        c = (1 - alpha) * np.cos(a0) + alpha * np.cos(a1)\n",
    "        s = (1 - alpha) * np.sin(a0) + alpha * np.sin(a1)\n",
    "        return np.arctan2(s, c)\n",
    "    \n",
    "def dict_to_device(d,device):\n",
    "        for key, item in d.items():\n",
    "                if type(item) == torch.Tensor:\n",
    "                        d[key] = item.to(device)\n",
    "                elif type(item) == dict:\n",
    "                        dict_to_device(item,device)\n",
    "                else:\n",
    "                        NotImplementedError\n",
    "\n",
    "def simulate(model, scenario: scenario_pb2.Scenario, processed_data: HeteroData, print_verbose_comments: bool = False)->tf.Tensor:\n",
    "        vprint = print if print_verbose_comments else lambda arg: None\n",
    "        # To load the data, we create a simple tensorized version of the object tracks.\n",
    "        logged_trajectories = trajectory_utils.ObjectTrajectories.from_scenario(scenario)\n",
    "\n",
    "        # Using `ObjectTrajectories` we can select just the objects that we need to\n",
    "        # simulate and remove the \"future\" part of the Scenario.\n",
    "        vprint(\n",
    "        'Original shape of tensors inside trajectories:'\n",
    "        f' {logged_trajectories.valid.shape} (n_objects, n_steps)'\n",
    "        )\n",
    "        logged_trajectories = logged_trajectories.gather_objects_by_id(\n",
    "        tf.convert_to_tensor(\n",
    "                submission_specs.get_sim_agent_ids(scenario, challenge_type)\n",
    "        )\n",
    "        )\n",
    "        logged_trajectories = logged_trajectories.slice_time(\n",
    "        start_index=0, end_index=submission_config.current_time_index + 1\n",
    "        )\n",
    "        vprint(\n",
    "        'Modified shape of tensors inside trajectories:'\n",
    "        f' {logged_trajectories.valid.shape} (n_objects, n_steps)'\n",
    "        )\n",
    "\n",
    "        # We can verify that all of these objects are valid at the last step.\n",
    "        vprint(\n",
    "        'Are all agents valid:'\n",
    "        f' {tf.reduce_all(logged_trajectories.valid[:, -1]).numpy()}'\n",
    "        )\n",
    "        #\n",
    "        #Simulate using our trained sim agent model\n",
    "        num_recurrent_steps_val = processed_data[\"agent\"]['position'].shape[1] - model.num_historical_steps\n",
    "        num_token_steps = num_recurrent_steps_val // model.encoder.agent_encoder.shift\n",
    "        num_agents = len(processed_data[\"agent\"]['id'])\n",
    "\n",
    "        simulated_trajectories = np.zeros((submission_config.n_rollouts,num_agents,submission_config.n_simulation_steps+1,4))\n",
    "\n",
    "        #Set the initial time step as the current state aka last logged trajectory (index 10)\n",
    "        simulated_trajectories[:,:,0,0] = logged_trajectories.x.numpy()[:,-1]\n",
    "        simulated_trajectories[:,:,0,1] = logged_trajectories.y.numpy()[:,-1]\n",
    "        simulated_trajectories[:,:,0,2] = logged_trajectories.z.numpy()[:,-1]\n",
    "        simulated_trajectories[:,:,0,3] = logged_trajectories.heading.numpy()[:,-1]\n",
    "\n",
    "        vprint('Generating 'f'{submission_config.n_rollouts} trajectories for scenario {scenario.scenario_id}')\n",
    "        dict_to_device(processed_data,model.device)\n",
    "        data_init = processed_data\n",
    "        z_average = processed_data['agent']['position'][:,:,-1].mean(dim=1)\n",
    "        shift = model.encoder.agent_encoder.shift\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "                for j in range(submission_config.n_rollouts):\n",
    "                        data = model.match_token_map(data_init)\n",
    "                        data = model.sample_pt_pred(data)\n",
    "                        state = model.inference_one_step_init(data)\n",
    "                        for t in range(num_token_steps):\n",
    "                                out = model.inference_rollout_step(state, t)\n",
    "                                pred_center = state[\"pos_a\"] #(x,y) position only\n",
    "                                hist_token_idx = state[\"hist_token_idx\"]\n",
    "                                heading = state['head_a']\n",
    "                                pred_z = z_average #average z coordinate of each agent across agent history\n",
    "\n",
    "                                #Copy the rollout states to trajectories\n",
    "                                simulated_trajectories[j,:,(t+1)*shift,:2] = pred_center[:,hist_token_idx +t,:].cpu().numpy()\n",
    "                                simulated_trajectories[j,:,(t+1)*shift, 2] = pred_z.cpu().numpy()\n",
    "                                simulated_trajectories[j,:,(t+1)*shift, 3] = wrap_angle(heading[:,hist_token_idx + t].cpu().numpy())\n",
    "\n",
    "                        # Since our motion token is discretized with dt=0.5s but the simulation must be in dt=0.1s, we do linear interpolation\n",
    "                        for k in range(shift,submission_config.n_simulation_steps+1,shift):\n",
    "                                ctr = 0\n",
    "                                for m in range(k-shift,k):\n",
    "                                        simulated_trajectories[j,:,m,:2] = simulated_trajectories[j,:,k-shift,:2] + (ctr/shift) * (simulated_trajectories[j,:,k,:2] - simulated_trajectories[j,:,k-shift,:2])\n",
    "                                        simulated_trajectories[j,:,m, 2] = simulated_trajectories[j,:,k-shift,2] + (ctr/shift) * (simulated_trajectories[j,:,k,2] - simulated_trajectories[j,:,k-shift,2])\n",
    "                                        # simulated_trajectories[j,:,m, 3] = wrap_angle(simulated_trajectories[j,:,k-shift,3] + (ctr/shift) * (simulated_trajectories[j,:,k,3] - simulated_trajectories[j,:,k-shift,3]))\n",
    "                                        simulated_trajectories[j, :, m, 3] = interp_angle_shortest(\n",
    "                                                                                simulated_trajectories[j, :, k-shift, 3],\n",
    "                                                                                simulated_trajectories[j, :, k,       3],\n",
    "                                                                                ctr / shift\n",
    "                                                                                )\n",
    "\n",
    "                                        ctr+=1\n",
    "                                        \n",
    "        simulated_trajectories_tf = tf.convert_to_tensor(simulated_trajectories[:,:,1:,:])\n",
    "        assert simulated_trajectories_tf.shape == (submission_config.n_rollouts,num_agents,cfg.time_info.num_future_steps,4)\n",
    "        return logged_trajectories, simulated_trajectories_tf\n",
    "\n",
    "def get_processed_scenario(scenario, dataset):\n",
    "        idx = dataset.query_index(scenario.scenario_id)\n",
    "        try:\n",
    "               return dataset.get(idx[0])\n",
    "        except:\n",
    "               print(f'Scenario ID: {scenario.scenario_id} could not be found')\n",
    "\n",
    "torch.manual_seed(2026)\n",
    "config_dir = '/home/hansung/end2end_ad/src/configs/validation/validation_rl.yaml'\n",
    "cfg = load_config_act(config_dir)\n",
    "\n",
    "#Load the model\n",
    "if cfg.finetuning.pretrain_ckpt:\n",
    "        print(f\"Loading pretrained model from: {cfg.finetuning.pretrain_ckpt}\")\n",
    "        model = SMART.load_from_checkpoint(\n",
    "                cfg.finetuning.pretrain_ckpt,\n",
    "                model_config=cfg.Model,   # <-- must match SMART.__init__ signature\n",
    "                strict=True,\n",
    "                )\n",
    "        model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "        NotImplementedError\n",
    "\n",
    "#Initialize the dataset\n",
    "if dataset_mode == 'val':\n",
    "        processed_dataset = MultiDataset(None, 'val', processed_dir=cfg.Dataset.val_processed_dir, raw_dir=cfg.Dataset.val_raw_dir, processor='ntp', transform=WaymoTargetBuilder(cfg.time_info.num_historical_steps, cfg.time_info.num_future_steps, \"val\"), token_size=cfg.time_info.token_size,smart_token=cfg.Model.use_smart_tokens)\n",
    "else:\n",
    "        processed_dataset = MultiDataset(None, 'test', processed_dir=cfg.Dataset.test_processed_dir, raw_dir=cfg.Dataset.test_raw_dir, processor='ntp', transform=WaymoTargetBuilder(cfg.time_info.num_historical_steps, cfg.time_info.num_future_steps), token_size=cfg.time_info.token_size,smart_token=cfg.Model.use_smart_tokens)\n",
    "\n",
    "# Process an example scenario\n",
    "print(scenario.scenario_id)\n",
    "processed_data = get_processed_scenario(scenario,processed_dataset) #get the correesponding processed data\n",
    "logged_trajectories, simulated_states = simulate(model, scenario, processed_data, True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched Rollout (if you have enough VRAM in your GPU)\n",
    "If you don't have enough VRAM and try to run this, it will result in OutOfMemoryError, which is fine. It just means that evaluation and submission generation will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData, DataLoader, Batch\n",
    "from dataset.dataset import MultiDataset\n",
    "from model.smart import SMART\n",
    "from utils.config import load_config_act\n",
    "from transforms.target_builder import WaymoTargetBuilder\n",
    "import torch\n",
    "from scipy.interpolate import CubicSpline\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def smooth_upsample_knots_to_0p1(traj, shift=5, dt=0.1):\n",
    "    \"\"\"\n",
    "    traj: (num_agents, T+1, 4) with knot values filled at indices 0, shift, 2*shift, ...\n",
    "          columns: [x, y, z, yaw]\n",
    "    Returns: same shape (num_agents, T+1, 4) fully filled with smooth 0.1s samples.\n",
    "    \"\"\"\n",
    "    num_agents, Tp1, D = traj.shape\n",
    "    assert D == 4\n",
    "    T = Tp1 - 1\n",
    "\n",
    "    knot_idx = np.arange(0, Tp1, shift)          # e.g. [0,5,10,...,80]\n",
    "    t_k = knot_idx * dt\n",
    "    t = np.arange(0, Tp1) * dt\n",
    "\n",
    "    out = traj.copy()\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        # --- x,y,z: C2-smooth spline (smooth velocity + acceleration) ---\n",
    "        for dim in (0, 1, 2):\n",
    "            yk = traj[i, knot_idx, dim]\n",
    "            cs = CubicSpline(t_k, yk, bc_type=\"natural\")\n",
    "            out[i, :, dim] = cs(t)\n",
    "\n",
    "        # --- yaw: spline sin/cos to avoid wrap discontinuity ---\n",
    "        yaw_k = traj[i, knot_idx, 3]\n",
    "        s_k = np.sin(yaw_k)\n",
    "        c_k = np.cos(yaw_k)\n",
    "\n",
    "        cs_s = CubicSpline(t_k, s_k, bc_type=\"natural\")\n",
    "        cs_c = CubicSpline(t_k, c_k, bc_type=\"natural\")\n",
    "\n",
    "        s = cs_s(t)\n",
    "        c = cs_c(t)\n",
    "\n",
    "        # normalize (keeps it on unit circle; helps avoid drift)\n",
    "        norm = np.maximum(np.sqrt(s*s + c*c), 1e-8)\n",
    "        s /= norm\n",
    "        c /= norm\n",
    "\n",
    "        out[i, :, 3] = np.arctan2(s, c)\n",
    "\n",
    "    return out\n",
    "\n",
    "def interp_angle_shortest(a0, a1, alpha):\n",
    "        # a0,a1: (...,) in radians, alpha scalar in [0,1]\n",
    "        c = (1 - alpha) * np.cos(a0) + alpha * np.cos(a1)\n",
    "        s = (1 - alpha) * np.sin(a0) + alpha * np.sin(a1)\n",
    "        return np.arctan2(s, c)\n",
    "    \n",
    "def dict_to_device(d,device):\n",
    "        for key, item in d.items():\n",
    "                if type(item) == torch.Tensor:\n",
    "                        d[key] = item.to(device)\n",
    "                elif type(item) == dict:\n",
    "                        dict_to_device(item,device)\n",
    "                else:\n",
    "                        NotImplementedError\n",
    "\n",
    "def batched_simulate(model, scenario, processed_data: HeteroData, print_verbose_comments: bool = False, batch_size: int = 1) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Simulate trajectories with optimized memory usage and vectorized interpolation.\n",
    "    \"\"\"\n",
    "    vprint = print if print_verbose_comments else lambda arg: None\n",
    "\n",
    "    # --- 1. Load Ground Truth Object Tracks ---\n",
    "    logged_trajectories = trajectory_utils.ObjectTrajectories.from_scenario(scenario)\n",
    "    vprint(f'Original shape: {logged_trajectories.valid.shape} (n_objects, n_steps)')\n",
    "\n",
    "    logged_trajectories = logged_trajectories.gather_objects_by_id(\n",
    "        tf.convert_to_tensor(submission_specs.get_sim_agent_ids(scenario, challenge_type))\n",
    "    )\n",
    "    logged_trajectories = logged_trajectories.slice_time(\n",
    "        start_index=0, end_index=submission_config.current_time_index + 1\n",
    "    )\n",
    "    vprint(f'Modified shape: {logged_trajectories.valid.shape}')\n",
    "\n",
    "    # --- 2. Setup DataLoader for Rollouts ---\n",
    "    # We duplicate the data reference N times. \n",
    "    # NOTE: processed_data is NOT copied in memory here, only references are added to the list.\n",
    "    data_list = [processed_data] * submission_config.n_rollouts\n",
    "    \n",
    "    # CRITICAL FIX: Use smaller batch_size (1 or 2) to fit in VRAM\n",
    "    loader = DataLoader(data_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # --- 3. Prepare Simulation Arrays ---\n",
    "    num_recurrent_steps_val = processed_data[\"agent\"]['position'].shape[1] - model.num_historical_steps\n",
    "    shift = model.encoder.agent_encoder.shift\n",
    "    # Calculate how many token steps we need to simulate\n",
    "    num_token_steps = submission_config.n_simulation_steps // shift\n",
    "    \n",
    "    num_agents = len(processed_data[\"agent\"]['id'])\n",
    "    \n",
    "    # Main storage for rollouts (on CPU RAM)\n",
    "    sim_traj = np.zeros((submission_config.n_rollouts, num_agents, submission_config.n_simulation_steps + 1, 4))\n",
    "\n",
    "    # Initialize t=0 with the last known state\n",
    "    sim_traj[:, :, 0, 0] = logged_trajectories.x.numpy()[:, -1]\n",
    "    sim_traj[:, :, 0, 1] = logged_trajectories.y.numpy()[:, -1]\n",
    "    sim_traj[:, :, 0, 2] = logged_trajectories.z.numpy()[:, -1]\n",
    "    sim_traj[:, :, 0, 3] = logged_trajectories.heading.numpy()[:, -1]\n",
    "\n",
    "    vprint(f'Generating {submission_config.n_rollouts} trajectories...')\n",
    "    \n",
    "    # Pre-calculate constants\n",
    "    z_average = processed_data['agent']['position'][:, :, -1].mean(dim=1).cpu().numpy() # (num_agents,)\n",
    "\n",
    "    # --- 4. Inference Loop ---\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    start_idx = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in loader:\n",
    "            current_bs = batch.num_graphs\n",
    "            \n",
    "            # Move batch to GPU\n",
    "            data = model.match_token_map(batch)\n",
    "            data = model.sample_pt_pred(data)\n",
    "            data = data.to(model.device)\n",
    "\n",
    "            # Initialize autoregressive state\n",
    "            state = model.inference_one_step_init(data)\n",
    "            \n",
    "            # Autoregressive Rollout\n",
    "            for t in range(num_token_steps):\n",
    "                model.inference_rollout_step(state, t)\n",
    "                \n",
    "                # Extract predictions\n",
    "                # shape: (current_bs * num_agents, 1, 2) -> reshape to (current_bs, num_agents, 2)\n",
    "                hist_token_idx = state[\"hist_token_idx\"]\n",
    "                \n",
    "                # Position (x, y)\n",
    "                pred_center = state[\"pos_a\"][:, hist_token_idx + t, :].cpu().numpy()\n",
    "                pred_center = pred_center.reshape(current_bs, num_agents, 2)\n",
    "                \n",
    "                # Heading\n",
    "                heading = state['head_a'][:, hist_token_idx + t].cpu().numpy()\n",
    "                heading = wrap_angle(heading).reshape(current_bs, num_agents)\n",
    "\n",
    "                # Store Knot Points (every 'shift' steps)\n",
    "                # target_idx matches the future time step: (t+1)*shift\n",
    "                target_t_idx = (t + 1) * shift\n",
    "                \n",
    "                if target_t_idx <= submission_config.n_simulation_steps:\n",
    "                    sim_traj[start_idx:start_idx+current_bs, :, target_t_idx, :2] = pred_center\n",
    "                    sim_traj[start_idx:start_idx+current_bs, :, target_t_idx, 2]  = z_average\n",
    "                    sim_traj[start_idx:start_idx+current_bs, :, target_t_idx, 3]  = heading\n",
    "\n",
    "            # --- 5. Vectorized Interpolation (CPU) ---\n",
    "            # Instead of nested loops, we interpolate the whole batch slice at once.\n",
    "            # We have values at 0, shift, 2*shift... we need to fill indices between them.\n",
    "            \n",
    "            for step in range(1, num_token_steps + 1):\n",
    "                t_end = step * shift\n",
    "                t_start = (step - 1) * shift\n",
    "                \n",
    "                if t_end > submission_config.n_simulation_steps:\n",
    "                    break\n",
    "\n",
    "                # Get start and end knots\n",
    "                # Shape: (current_bs, num_agents, 4)\n",
    "                p_start = sim_traj[start_idx:start_idx+current_bs, :, t_start, :]\n",
    "                p_end   = sim_traj[start_idx:start_idx+current_bs, :, t_end, :]\n",
    "\n",
    "                # Create interpolation weights\n",
    "                # alpha shape: (shift-1, 1, 1, 1) or broadcastable\n",
    "                alphas = np.linspace(0, 1, shift + 1)[1:-1] # excludes 0 and 1\n",
    "                \n",
    "                # Linear Interp for X, Y, Z\n",
    "                # Result shape: (shift-1, current_bs, num_agents, 3)\n",
    "                # We transpose to align with sim_traj: (current_bs, num_agents, shift-1, 3)\n",
    "                interp_xyz = (1 - alphas[:, None, None, None]) * p_start[:, :, :3] + \\\n",
    "                             (alphas[:, None, None, None]) * p_end[:, :, :3]\n",
    "                interp_xyz = interp_xyz.transpose(1, 2, 0, 3)\n",
    "\n",
    "                # Slerp/Shortest Angle Interp for Yaw\n",
    "                # Using the provided helper function logic but vectorized\n",
    "                a0 = p_start[:, :, 3]\n",
    "                a1 = p_end[:, :, 3]\n",
    "                \n",
    "                # Vectorized angle interpolation\n",
    "                # alphas: (shift-1) -> expand for broadcast\n",
    "                alphas_expanded = alphas[:, None, None] # (steps, 1, 1)\n",
    "                c = (1 - alphas_expanded) * np.cos(a0) + alphas_expanded * np.cos(a1)\n",
    "                s = (1 - alphas_expanded) * np.sin(a0) + alphas_expanded * np.sin(a1)\n",
    "                interp_yaw = np.arctan2(s, c) # (shift-1, bs, agents)\n",
    "                interp_yaw = interp_yaw.transpose(1, 2, 0) # (bs, agents, shift-1)\n",
    "\n",
    "                # Assign back to main array\n",
    "                sim_traj[start_idx:start_idx+current_bs, :, t_start+1 : t_end, :3] = interp_xyz\n",
    "                sim_traj[start_idx:start_idx+current_bs, :, t_start+1 : t_end, 3]  = interp_yaw\n",
    "\n",
    "            # Clean up GPU memory for next batch\n",
    "            del data, state\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            start_idx += current_bs\n",
    "\n",
    "    # Convert to TF tensor as required\n",
    "    simulated_trajectories_tf = tf.convert_to_tensor(sim_traj[:, :, 1:, :], dtype=tf.float32)\n",
    "    return logged_trajectories, simulated_trajectories_tf\n",
    "\n",
    "def get_processed_scenario(scenario, dataset):\n",
    "        idx = dataset.query_index(scenario.scenario_id)\n",
    "        try:\n",
    "               return dataset.get(idx[0])\n",
    "        except:\n",
    "               print(f'Scenario ID: {scenario.scenario_id} could not be found')\n",
    "\n",
    "torch.manual_seed(2026)\n",
    "config_dir = '/home/hansung/end2end_ad/src/configs/validation/validation_rl.yaml'\n",
    "cfg = load_config_act(config_dir)\n",
    "\n",
    "#Load the model\n",
    "if cfg.finetuning.pretrain_ckpt:\n",
    "        print(f\"Loading pretrained model from: {cfg.finetuning.pretrain_ckpt}\")\n",
    "        model = SMART.load_from_checkpoint(\n",
    "                cfg.finetuning.pretrain_ckpt,\n",
    "                model_config=cfg.Model,   # <-- must match SMART.__init__ signature\n",
    "                strict=True,\n",
    "                )\n",
    "        model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "        NotImplementedError\n",
    "\n",
    "#Initialize the dataset\n",
    "if dataset_mode == 'val':\n",
    "        processed_dataset = MultiDataset(None, 'val', processed_dir=cfg.Dataset.val_processed_dir, raw_dir=cfg.Dataset.val_raw_dir, processor='ntp', transform=WaymoTargetBuilder(cfg.time_info.num_historical_steps, cfg.time_info.num_future_steps, \"val\"), token_size=cfg.time_info.token_size,smart_token=cfg.Model.use_smart_tokens)\n",
    "else:\n",
    "        processed_dataset = MultiDataset(None, 'test', processed_dir=cfg.Dataset.test_processed_dir, raw_dir=cfg.Dataset.test_raw_dir, processor='ntp', transform=WaymoTargetBuilder(cfg.time_info.num_historical_steps, cfg.time_info.num_future_steps), token_size=cfg.time_info.token_size,smart_token=cfg.Model.use_smart_tokens)\n",
    "\n",
    "# Process an example scenario\n",
    "print(scenario.scenario_id)\n",
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "processed_data = get_processed_scenario(scenario,processed_dataset) #get the correesponding processed data\n",
    "logged_trajectories, simulated_states = batched_simulate(model, scenario, HeteroData(processed_data), True,batch_size=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQlNA6WCm4OX"
   },
   "source": [
    "### Visualize the simulated trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hI_Qbh2cMJfQ"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "\n",
    "# Select which how the 32 simulations we want visualize.\n",
    "os.makedirs(f'../demo/{scenario.scenario_id}', exist_ok=True)\n",
    "for i in range(submission_config.n_rollouts):\n",
    "    SAMPLE_INDEX = i\n",
    "    # We need to extract box sizes (length and width) for all the simulated objects,\n",
    "    # from the 11th step (when one-indexed) of the original scenario.\n",
    "    # Also broadcast in time to be compatible with the other tensors,\n",
    "    # shape (num_objects, num_steps).\n",
    "    n_objects = logged_trajectories.valid.shape[0]\n",
    "    lengths = tf.broadcast_to(\n",
    "        logged_trajectories.length[:, 10, tf.newaxis],\n",
    "        (n_objects, submission_config.n_simulation_steps),\n",
    "    )\n",
    "    widths = tf.broadcast_to(\n",
    "        logged_trajectories.width[:, 10, tf.newaxis],\n",
    "        (n_objects, submission_config.n_simulation_steps),\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    anim = visualizations.get_animated_states(\n",
    "        fig,\n",
    "        ax,\n",
    "        scenario,\n",
    "        simulated_states[SAMPLE_INDEX, :, :, 0],\n",
    "        simulated_states[SAMPLE_INDEX, :, :, 1],\n",
    "        simulated_states[SAMPLE_INDEX, :, :, 3],\n",
    "        lengths,\n",
    "        widths,\n",
    "        color_idx=tf.zeros((70, 80), dtype=tf.int32),\n",
    "    )\n",
    "\n",
    "    cx = logged_trajectories.x[processed_data['agent']['av_index'],0].numpy()\n",
    "    cy = logged_trajectories.y[processed_data['agent']['av_index'],0].numpy()\n",
    "    dist = 130\n",
    "    ax.set_xlim(cx - dist,cx+dist)\n",
    "    ax.set_ylim(cy - dist, cy + dist)\n",
    "    # Make x/y have the same scale\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")   # or adjustable=\"datalim\"\n",
    "\n",
    "    # UNCOMMENT IF YOU WANT TO VISUALIZE A SAMPLE IN THE NOTEBOOK and don't iterate (for loop)\n",
    "    # HTML(anim.to_jshtml())\n",
    "\n",
    "    writer = FFMpegWriter(fps=10, bitrate=2000)\n",
    "    anim.save(f\"../demo/{scenario.scenario_id}/validation_{scenario.scenario_id}_{SAMPLE_INDEX}.mp4\", writer=writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_states[SAMPLE_INDEX, :, :, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAHXM4jm29E_"
   },
   "outputs": [],
   "source": [
    "def joint_scene_from_states(\n",
    "    states: tf.Tensor, object_ids: tf.Tensor\n",
    ") -> sim_agents_submission_pb2.JointScene:\n",
    "  # States shape: (num_objects, num_steps, 4).\n",
    "  # Objects IDs shape: (num_objects,).\n",
    "  states = states.numpy()\n",
    "  simulated_trajectories = []\n",
    "  for i_object in range(len(object_ids)):\n",
    "    simulated_trajectories.append(\n",
    "        sim_agents_submission_pb2.SimulatedTrajectory(\n",
    "            center_x=states[i_object, :, 0],\n",
    "            center_y=states[i_object, :, 1],\n",
    "            center_z=states[i_object, :, 2],\n",
    "            heading=states[i_object, :, 3],\n",
    "            object_id=object_ids[i_object],\n",
    "        )\n",
    "    )\n",
    "  return sim_agents_submission_pb2.JointScene(\n",
    "      simulated_trajectories=simulated_trajectories\n",
    "  )\n",
    "\n",
    "\n",
    "# Package the first simulation into a `JointScene`\n",
    "joint_scene = joint_scene_from_states(\n",
    "    simulated_states[0, :, :, :], logged_trajectories.object_id\n",
    ")\n",
    "# Validate the joint scene. Should raise an exception if it's invalid.\n",
    "submission_specs.validate_joint_scene(joint_scene, scenario, challenge_type)\n",
    "\n",
    "\n",
    "# Now we can replicate this strategy to export all the parallel simulations.\n",
    "def scenario_rollouts_from_states(\n",
    "    scenario: scenario_pb2.Scenario, states: tf.Tensor, object_ids: tf.Tensor\n",
    ") -> sim_agents_submission_pb2.ScenarioRollouts:\n",
    "  # States shape: (num_rollouts, num_objects, num_steps, 4).\n",
    "  # Objects IDs shape: (num_objects,).\n",
    "  joint_scenes = []\n",
    "  for i_rollout in range(states.shape[0]):\n",
    "    joint_scenes.append(joint_scene_from_states(states[i_rollout], object_ids))\n",
    "  return sim_agents_submission_pb2.ScenarioRollouts(\n",
    "      # Note: remember to include the Scenario ID in the proto message.\n",
    "      joint_scenes=joint_scenes,\n",
    "      scenario_id=scenario.scenario_id,\n",
    "  )\n",
    "\n",
    "\n",
    "scenario_rollouts = scenario_rollouts_from_states(\n",
    "    scenario, simulated_states, logged_trajectories.object_id\n",
    ")\n",
    "# As before, we can validate the message we just generate.\n",
    "submission_specs.validate_scenario_rollouts(scenario_rollouts, scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EesEPDXdPZh"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Once we have created the submission for a single Scenario, we can evaluate the\n",
    "simulations we have performed.\n",
    "\n",
    "The evaluation of sim agents is trying to capture distributional realism, i.e.\n",
    "how well our simulations capture the distribution of human behaviour from the\n",
    "real world. A key difference to the existing Behaviour Prediction task, is that\n",
    "we are focusing our comparison on quantities (**features**) that try to capture\n",
    "the behaviour of humans.\n",
    "\n",
    "More specifically, for this challenge we will look at the following features: -\n",
    "Kinematic features: speed / accelerations of objects, both linear and angular. -\n",
    "Interactive features: features capturing relationships between objects, like\n",
    "collisions, distances to other objects and time to collision (TTC). - Map-based\n",
    "features: features capturing how objects move with respect to the road itself,\n",
    "e.g. going offroad for a car.\n",
    "\n",
    "While we require all those objects to be simulated, we are going to evaluate\n",
    "only a subset of them, namely the `tracks_to_predict` inside the Scenario. This\n",
    "criteria was put in place to ensure less noisy measures, as these objects will\n",
    "have consistently long observations from the real world, which we need to\n",
    "properly evaluate our agents.\n",
    "\n",
    "Note that, while all the other sim agents are not *directly* evaluated, they are\n",
    "still part of the simulation. This means that all the interactive features will\n",
    "be computed considering those sim agents, and the *evaluated* sim agents needs\n",
    "to be reactive to these objects.\n",
    "\n",
    "Now let's compute the features to understand better the evaluation in practice.\n",
    "Everything is included inside `metric_features.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpqTJ6oNoWoE"
   },
   "outputs": [],
   "source": [
    "if dataset_mode == 'val':\n",
    "    # Compute the features for a single JointScene.\n",
    "    single_scene_features = metric_features.compute_metric_features(\n",
    "        scenario, joint_scene\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jopGed89EmGf"
   },
   "outputs": [],
   "source": [
    "if dataset_mode == 'val':  \n",
    "  # These features will be computed only for the `tracks_to_predict` objects.\n",
    "  print(\n",
    "      'Evaluated objects:'\n",
    "      f' {submission_specs.get_evaluation_sim_agent_ids(scenario, challenge_type)}'\n",
    "  )\n",
    "  # This will also match single_scene_features.object_ids\n",
    "  print(f'Evaluated objects in features: {single_scene_features.object_id}')\n",
    "\n",
    "  # Features contain a validity flag, which for simulated rollouts must be always\n",
    "  # True, because we are requiring the sim agents to be always valid when replaced.\n",
    "  print(f'Are all agents valid: {tf.reduce_all(single_scene_features.valid)}')\n",
    "\n",
    "  # ============ FEATURES ============\n",
    "  # Average displacement feature. This corresponds to ADE in the BP challenges,\n",
    "  # here is used just as a comparison (it's not actually included in the final score).\n",
    "  # Shape: (1, n_objects).\n",
    "  print(\n",
    "      f'ADE: {tf.reduce_mean(single_scene_features.average_displacement_error)}'\n",
    "  )\n",
    "\n",
    "  # Kinematic features.\n",
    "  print('\\n============ KINEMATIC FEATURES ============')\n",
    "  fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "  for i_object in range(len(single_scene_features.object_id)):\n",
    "    _object_id = single_scene_features.object_id[i_object].numpy()\n",
    "    axes[0].plot(\n",
    "        single_scene_features.linear_speed[0, i_object, :], label=str(_object_id)\n",
    "    )\n",
    "    axes[1].plot(\n",
    "        single_scene_features.linear_acceleration[0, i_object, :],\n",
    "        label=str(_object_id),\n",
    "    )\n",
    "    axes[2].plot(\n",
    "        single_scene_features.angular_speed[0, i_object, :], label=str(_object_id)\n",
    "    )\n",
    "    axes[3].plot(\n",
    "        single_scene_features.angular_acceleration[0, i_object, :],\n",
    "        label=str(_object_id),\n",
    "    )\n",
    "\n",
    "\n",
    "  TITLES = [\n",
    "      'linear_speed',\n",
    "      'linear_acceleration',\n",
    "      'angular_speed',\n",
    "      'angular_acceleration',\n",
    "  ]\n",
    "  for ax, title in zip(axes, TITLES):\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "  plt.show()\n",
    "\n",
    "  # Interactive features.\n",
    "  print('\\n============ INTERACTIVE FEATURES ============')\n",
    "  print(f'Colliding objects: {single_scene_features.collision_per_step[0]}')\n",
    "  fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "  for i_object in range(len(single_scene_features.object_id)):\n",
    "    _object_id = single_scene_features.object_id[i_object].numpy()\n",
    "    axes[0].plot(\n",
    "        single_scene_features.distance_to_nearest_object[0, i_object, :],\n",
    "        label=str(_object_id),\n",
    "    )\n",
    "    axes[1].plot(\n",
    "        single_scene_features.time_to_collision[0, i_object, :],\n",
    "        label=str(_object_id),\n",
    "    )\n",
    "\n",
    "  TITLES = ['distance to nearest object', 'time to collision']\n",
    "  for ax, title in zip(axes, TITLES):\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "  plt.show()\n",
    "\n",
    "  # Map-based features.\n",
    "  print('\\n============ MAP-BASED FEATURES ============')\n",
    "  print(f'Offroad objects: {single_scene_features.offroad_per_step[0]}')\n",
    "  fig, axes = plt.subplots(1, 1, figsize=(4, 4))\n",
    "  for i_object in range(len(single_scene_features.object_id)):\n",
    "    _object_id = single_scene_features.object_id[i_object].numpy()\n",
    "    axes.plot(\n",
    "        single_scene_features.distance_to_road_edge[0, i_object, :],\n",
    "        label=str(_object_id),\n",
    "    )\n",
    "  axes.legend()\n",
    "  axes.set_title('distance to road edge')\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYzJ0HevRbI8"
   },
   "source": [
    "These features are computed for each of the submitted `JointScenes`. So, for a\n",
    "given `ScenarioRollouts` we actually get a distribution of these features over\n",
    "the parallel rollouts.\n",
    "\n",
    "The final metric we will be evaluating is a measure of the likelihood of what\n",
    "happened in real life, compared to the distribution of what *we predicted might\n",
    "have happened* (in simulation). For more details see the challenge\n",
    "documentation.\n",
    "\n",
    "The final metrics can be called directly from `metrics.py`, as shown below.\n",
    "\n",
    "Some of the details of how these metrics are computed and aggregated can be\n",
    "found in `SimAgentMetricsConfig`. The following code demonstrates how to load\n",
    "the config used for the challenge and how to score your own submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nbLEo0aSKhK"
   },
   "outputs": [],
   "source": [
    "if dataset_mode == 'val':\n",
    "    # Load the test configuration.\n",
    "    config = metrics.load_metrics_config(challenge_type)\n",
    "\n",
    "    scenario_metrics = metrics.compute_scenario_metrics_for_bundle(\n",
    "        config, scenario, scenario_rollouts\n",
    "    )\n",
    "    print(scenario_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cwPMiJjS-TT"
   },
   "source": [
    "As you can see, there is a score in the range [0,1] for each of the features\n",
    "listed above. The new field to highlight is `metametric`: this is a linear\n",
    "combination of the per-feature scores, and it's the final metric used to score\n",
    "and rank submissions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
